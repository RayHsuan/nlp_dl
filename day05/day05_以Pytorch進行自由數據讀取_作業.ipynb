{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "day05_以Pytorch進行自由數據讀取_作業.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8YthK84P-Ks"
      },
      "source": [
        "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
        "\n",
        "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
        "客製化資料讀取。\n",
        "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
        "(請同學先行至IMDB下載資料)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IirW03bvP-Kz"
      },
      "source": [
        "### 載入套件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIhOmHr7P-Kz",
        "outputId": "0f93daca-91a6-4919-c3f4-3cfedf40c2dc"
      },
      "source": [
        "# Import torch and other required modules\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') #下載stopwords\n",
        "nltk.download('punkt') #下載word_tokenize需要的corpus"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfyX82D0QqI2",
        "outputId": "aebe67cd-178e-4227-8451-fc29deacd36c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQBirmKmP-K1"
      },
      "source": [
        "### 探索資料與資料前處理\n",
        "這份作業我們使用test資料中的pos與neg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77E5wYBwP-K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57ea9ca-1e26-4784-b096-fd51db044289"
      },
      "source": [
        "# 讀取字典，這份字典為review內所有出現的字詞\n",
        "df_vocab = pd.read_csv('./gdrive/My Drive/nlp2_colab/data/aclImdb/imdb.vocab', names=['voc'], encoding=\"latin\")\n",
        "vocab = df_vocab.voc.tolist()\n",
        "\n",
        "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
        "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
        "vocab=[word for word in vocab if not word in set(stopwords.words('english'))]\n",
        "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
        "\n",
        "# 將字典轉換成dictionary\n",
        "vocab_list = list(set(vocab))\n",
        "vocab_list = [x for x in vocab_list if str(x) != 'nan']\n",
        "vocab_dic = {}\n",
        "for idx, word in enumerate(vocab_list):\n",
        "  vocab_dic[word] = idx"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab length before removing stopwords: 89527\n",
            "vocab length after removing stopwords: 89356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5U_MAaLQjjo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om0YcGR9P-K2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc70d26-025f-4ec7-d313-cf67d4f28050"
      },
      "source": [
        "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
        "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
        "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
        "path = './gdrive/My Drive/nlp2_colab/data/aclImdb/train/'\n",
        "review_pos = glob.glob(path + \"pos/*.txt\")\n",
        "review_neg = glob.glob(path + \"neg/*.txt\")\n",
        "review_all = review_pos + review_neg\n",
        "y = [1]*len(review_pos) + [0]*len(review_neg)\n",
        "\n",
        "review_pairs = list(zip(review_all, y))\n",
        "print(review_pairs[:2])\n",
        "print(f\"Total reviews: {len(review_pairs)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('./gdrive/My Drive/nlp2_colab/data/aclImdb/train/pos/11414_9.txt', 1), ('./gdrive/My Drive/nlp2_colab/data/aclImdb/train/pos/11609_10.txt', 1)]\n",
            "Total reviews: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2MwJni7P-K2"
      },
      "source": [
        "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
        "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
        "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8hAZ2AP-K2"
      },
      "source": [
        "def load_review(review_path):\n",
        "    \n",
        "  with open(review_path, 'r') as f:\n",
        "    review = f.read()\n",
        "        \n",
        "  #移除non-alphabet符號、贅字與tokenize\n",
        "  review = re.sub('[^a-zA-Z]',' ',review)\n",
        "  review = nltk.word_tokenize(review)\n",
        "  review = list(set(review).difference(set(stopwords.words('english'))))\n",
        "  \n",
        "  return review\n",
        "  \n",
        "def generate_vec(review, vocab_dic):\n",
        "  doc_vec = []\n",
        "  for word in review:\n",
        "    if vocab_dic.get(word):\n",
        "      doc_vec.append(vocab_dic.get(word))\n",
        "            \n",
        "  return torch.tensor(doc_vec)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwxFmxdBP-K3"
      },
      "source": [
        "#建立客製化dataset\n",
        "\n",
        "class dataset(Dataset):\n",
        "    '''custom dataset to load reviews and labels\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_pairs: list\n",
        "        directory of all review-label pairs\n",
        "    vocab: list\n",
        "        list of vocabularies\n",
        "    '''\n",
        "    def __init__(self, data_dir, vocab):\n",
        "      self.data_dir = data_dir\n",
        "      self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data_dir)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      pair = self.data_dir[idx]\n",
        "      review = pair[0]\n",
        "      review = load_review(review)\n",
        "      review = generate_vec(review, self.vocab)\n",
        "        \n",
        "      return review, pair[1]  \n",
        "\n",
        "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
        "def collate_fn(batch):\n",
        "\n",
        "  corpus, labels = zip(*batch) \n",
        "    \n",
        "  ### create pads for corpus ###\n",
        "  lengths = [len(x) for x in corpus]\n",
        "  max_length = max(lengths)\n",
        "    \n",
        "  batch_corpus = []\n",
        "    \n",
        "  for i in range(len(corpus)):\n",
        "    # pad corpus\n",
        "    tmp_pads = torch.zeros(max_length)\n",
        "    tmp_pads[:lengths[i]] = corpus[i]\n",
        "    tmp_pads.view(-1, 1)\n",
        "    batch_corpus.append(tmp_pads.view(1,-1))\n",
        "\n",
        "  return torch.cat(batch_corpus,dim=0), torch.tensor(labels) , torch.tensor(lengths)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TwB3G5v5P-K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4775529-f83b-4b0b-a565-ffc9f781c04f"
      },
      "source": [
        "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
        "custom_dst = dataset(review_pairs, vocab_dic)\n",
        "custom_dataloader = DataLoader(dataset=custom_dst, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "next(iter(custom_dataloader))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[16510., 70393.,  3910., 23364., 25536., 75150., 29338., 14753., 47747.,\n",
              "          62957., 30014., 80573., 10593.,  1825., 79515., 10435., 32776., 36519.,\n",
              "          41612., 85239., 27325., 76288., 61755., 77846., 72604., 49511., 30070.,\n",
              "          39989., 35338., 88914., 18731., 75954., 89069., 61045., 10312., 65968.,\n",
              "           8035., 65633., 85940.,  3460., 39830., 81911.,  4903., 73164., 87549.,\n",
              "          27903., 66847., 68762., 16436., 33201.,  7130.,  5955.,  8604., 46500.,\n",
              "          11386., 72019., 28425., 34523., 17049., 45585., 51196., 63782., 67075.,\n",
              "          80352., 77760., 16287., 73197., 80162., 38852., 76388., 77422., 10398.,\n",
              "            691.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
              "         [44233., 50721., 16903., 48090., 75150.,  3196., 44432., 48447.,  4299.,\n",
              "          21009., 36184., 34948., 65417., 36519., 12499., 19252., 64602., 20318.,\n",
              "          85407., 66270., 49307., 86429., 41613., 76288., 43934., 20505., 72604.,\n",
              "          16393., 21862., 24592., 36219.,  1865.,  2755.,  9818., 58152.,  8208.,\n",
              "          10312., 13251., 35513.,  8035., 81316., 60189., 18224., 65633., 66665.,\n",
              "          62546., 21566.,  9645., 25277., 40549., 87023., 28918.,  7129., 16436.,\n",
              "          65322., 60557., 78410., 13474., 46500., 28425., 17049.,  4408., 60068.,\n",
              "          11913., 37264., 31528., 59046.,  1574.,  9179., 15244., 48064., 36124.,\n",
              "          42100., 35075., 53095., 15253., 72370., 67270., 42801., 43351.],\n",
              "         [57673., 16699., 23364., 49788., 50451.,   348., 85239., 29338., 66506.,\n",
              "          55588.,  8355., 57086., 72604., 76296., 85899.,  7220., 52392.,  4013.,\n",
              "          60326., 26309., 21817., 26227., 53146., 89069., 47494., 42577., 41181.,\n",
              "          58877., 60786., 38765.,  5111., 18116., 49385., 10312., 46314., 43509.,\n",
              "          32776., 46367., 26951., 65679., 57158.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.],\n",
              "         [86552., 80345., 77133., 22363., 50068., 23364., 12046., 34008., 29981.,\n",
              "          30545., 79538., 80352.,  3864., 56311., 85655., 13496.,  7973., 49269.,\n",
              "          76935., 69953., 70009., 74190., 85624., 73241., 74141., 84830., 86189.,\n",
              "          56686., 73299., 77198., 23712., 82011., 82101., 28324., 59587., 60326.,\n",
              "          11972., 32479., 21745., 35075., 77697., 20440., 43423., 29963., 38765.,\n",
              "          59108., 10312., 39289., 32776., 46367., 81081., 43351., 65679., 34618.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]]),\n",
              " tensor([1, 1, 0, 1]),\n",
              " tensor([73, 80, 41, 54]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_92T95dVP-K3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}